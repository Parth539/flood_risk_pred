{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca083dc6-cb3a-44ba-8d2e-dd819aa16721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import os\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project='parth-362005')\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project='parth-362005')\n",
    "\n",
    "\n",
    "aoi = ee.Geometry.Polygon([\n",
    "    [[90.0, 24.0], [90.5, 24.0], [90.5, 24.5], [90.0, 24.5], [90.0, 24.0]]\n",
    "])\n",
    "\n",
    "# Define time period \n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# Digital Elevation Model\n",
    "dem = ee.Image(\"USGS/SRTMGL1_003\").clip(aoi)\n",
    "\n",
    "# Calculate slope and aspect (important topographic features for flood modeling)\n",
    "slope = ee.Terrain.slope(dem).clip(aoi)\n",
    "aspect = ee.Terrain.aspect(dem).clip(aoi)\n",
    "\n",
    "# Rainfall data - both mean and maximum (for extreme events)\n",
    "rainfall_mean = ee.ImageCollection(\"ECMWF/ERA5_LAND/DAILY_AGGR\") \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .select(\"total_precipitation_sum\") \\\n",
    "    .mean().clip(aoi)\n",
    "\n",
    "rainfall_max = ee.ImageCollection(\"ECMWF/ERA5_LAND/DAILY_AGGR\") \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .select(\"total_precipitation_sum\") \\\n",
    "    .max().clip(aoi)\n",
    "\n",
    "# Soil moisture\n",
    "soil_moisture = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/006\") \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .select(\"soil_moisture\") \\\n",
    "    .mean().clip(aoi)\n",
    "\n",
    "# Land cover\n",
    "land_cover = ee.ImageCollection(\"MODIS/061/MCD12Q1\") \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .select(\"LC_Type1\") \\\n",
    "    .mode().clip(aoi)\n",
    "\n",
    "# Sentinel-1 for flood detection (VV and VH polarization)\n",
    "sentinel1 = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
    "    .filterBounds(aoi) \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\")) \\\n",
    "    .mean().clip(aoi)\n",
    "\n",
    "# NDVI for vegetation cover from Sentinel-2\n",
    "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n",
    "    .filterBounds(aoi) \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "    .median().clip(aoi)\n",
    "\n",
    "ndvi = s2.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "\n",
    "# Water occurrence (from JRC Global Surface Water)\n",
    "water_occurrence = ee.Image(\"JRC/GSW1_3/GlobalSurfaceWater\").select('occurrence').clip(aoi)\n",
    "\n",
    "# Population density for vulnerability assessment\n",
    "population = ee.ImageCollection(\"CIESIN/GPWv411/GPW_Population_Density\") \\\n",
    "    .filter(ee.Filter.date('2020-01-01', '2020-12-31')) \\\n",
    "    .mean().clip(aoi)\n",
    "\n",
    "# Export all layers to Google Drive\n",
    "export_tasks = {\n",
    "    \"DEM\": dem,\n",
    "    \"Slope\": slope,\n",
    "    \"Aspect\": aspect,\n",
    "    \"Rainfall_Mean\": rainfall_mean,\n",
    "    \"Rainfall_Max\": rainfall_max,\n",
    "    \"Soil_Moisture\": soil_moisture,\n",
    "    \"Land_Cover\": land_cover,\n",
    "    \"Sentinel1_VV\": sentinel1.select('VV'),\n",
    "    \"NDVI\": ndvi,\n",
    "    \"Water_Occurrence\": water_occurrence,\n",
    "    \"Population\": population\n",
    "}\n",
    "\n",
    "# Start export tasks\n",
    "for name, image in export_tasks.items():\n",
    "    task = ee.batch.Export.image.toDrive(\n",
    "        image=image,\n",
    "        description=f\"{name}_Flood_Risk\",\n",
    "        folder=\"FloodRiskData\",\n",
    "        fileNamePrefix=name.lower(),\n",
    "        scale=30,\n",
    "        region=aoi,\n",
    "        fileFormat=\"GeoTIFF\",\n",
    "        maxPixels=1e13  # Increased max pixels for larger areas\n",
    "    )\n",
    "    task.start()\n",
    "\n",
    "print(\"✅ Enhanced data export started successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45c1de-282a-464f-a9d9-c1a07b124c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_raster(file_path):\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = src.read(1)\n",
    "            data = data.astype(np.float32)\n",
    "            # Handle no data values\n",
    "            if src.nodata is not None:\n",
    "                data[data == src.nodata] = np.nan\n",
    "            # Replace NaNs with the mean of valid values\n",
    "            if np.any(np.isnan(data)):\n",
    "                valid_data = data[~np.isnan(data)]\n",
    "                if len(valid_data) > 0:\n",
    "                    data = np.nan_to_num(data, nan=np.mean(valid_data))\n",
    "                else:\n",
    "                    data = np.nan_to_num(data)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all raster datasets\n",
    "raster_files = {\n",
    "    \"dem\": \"dem.tif\",\n",
    "    \"slope\": \"slope.tif\",\n",
    "    \"aspect\": \"aspect.tif\",\n",
    "    \"rainfall_mean\": \"rainfall_mean.tif\",\n",
    "    \"rainfall_max\": \"rainfall_max.tif\",\n",
    "    \"soil_moisture\": \"soil_moisture.tif\",\n",
    "    \"land_cover\": \"land_cover.tif\",\n",
    "    \"sentinel1_vv\": \"sentinel1_vv.tif\",\n",
    "    \"ndvi\": \"ndvi.tif\",\n",
    "    \"water_occurrence\": \"water_occurrence.tif\",\n",
    "    \"population\": \"population.tif\",\n",
    "    \"flood_mask\": \"flood_mask.tif\"  # Historical flood mask if available\n",
    "}\n",
    "\n",
    "# Load all available datasets\n",
    "datasets = {}\n",
    "for name, file_path in raster_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        datasets[name] = load_raster(file_path)\n",
    "        print(f\"Loaded {name}\")\n",
    "    else:\n",
    "        print(f\"Warning: {file_path} not found, skipping.\")\n",
    "\n",
    "# Find minimum dimensions across all datasets\n",
    "shapes = [dataset.shape for dataset in datasets.values() if dataset is not None]\n",
    "min_height = min(shape[0] for shape in shapes)\n",
    "min_width = min(shape[1] for shape in shapes)\n",
    "\n",
    "# Crop all datasets to the same dimensions\n",
    "for name in datasets:\n",
    "    if datasets[name] is not None:\n",
    "        datasets[name] = datasets[name][:min_height, :min_width]\n",
    "\n",
    "# Normalize continuous data (not categorical data)\n",
    "for name in datasets:\n",
    "    if name != \"land_cover\" and name != \"flood_mask\" and datasets[name] is not None:\n",
    "        min_val = np.min(datasets[name])\n",
    "        max_val = np.max(datasets[name])\n",
    "        if max_val > min_val:  # Avoid division by zero\n",
    "            datasets[name] = (datasets[name] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Create feature stack (X) and target (y)\n",
    "feature_names = [name for name in datasets if name != \"flood_mask\"]\n",
    "X = np.stack([datasets[name] for name in feature_names if datasets[name] is not None], axis=-1)\n",
    "\n",
    "# If we have a flood mask, use it; otherwise use water occurrence as proxy\n",
    "if \"flood_mask\" in datasets and datasets[\"flood_mask\"] is not None:\n",
    "    y = (datasets[\"flood_mask\"] > 0).astype(np.uint8)\n",
    "else:\n",
    "    # Use water occurrence as proxy for flood risk\n",
    "    y = (datasets[\"water_occurrence\"] > 25).astype(np.uint8)\n",
    "\n",
    "y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "\n",
    "# Extract patches with improved patch extraction (with overlap for better coverage)\n",
    "patch_size = 256\n",
    "stride = 128\n",
    "\n",
    "# Function to extract patches\n",
    "def extract_patches(image, patch_size, stride):\n",
    "    patches = []\n",
    "    locations = []  # Store patch coordinates for visualization later\n",
    "\n",
    "    max_row = max(0, image.shape[0] - patch_size + 1)\n",
    "    max_col = max(0, image.shape[1] - patch_size + 1)\n",
    "\n",
    "    for i in range(0, max_row, stride):\n",
    "        for j in range(0, max_col, stride):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            # Skip mostly empty patches (e.g., patches with very few flood pixels)\n",
    "            if np.mean(patch) > 0.0001:  # Adjust threshold as needed\n",
    "                patches.append(patch)\n",
    "                locations.append((i, j))\n",
    "    while len(patches) == 0 and patch_size >= 32:  # Minimum patch size of 32\n",
    "        print(f\"No patches found with patch_size={patch_size}, stride={stride}. Reducing size...\")\n",
    "        patch_size //= 2  # Reduce patch size by half\n",
    "        stride //= 2  # Reduce stride by half\n",
    "\n",
    "        max_row = max(0, image.shape[0] - patch_size + 1)\n",
    "        max_col = max(0, image.shape[1] - patch_size + 1)\n",
    "\n",
    "        patches = []  # Reset patches list\n",
    "        locations = []  # Reset locations list\n",
    "\n",
    "        for i in range(0, max_row, stride):\n",
    "            for j in range(0, max_col, stride):\n",
    "                patch = image[i:i + patch_size, j:j + patch_size]\n",
    "                if np.mean(patch) > 0.0001:  # Adjust threshold as needed\n",
    "                    patches.append(patch)\n",
    "                    locations.append((i, j))\n",
    "\n",
    "    if len(patches) == 0:\n",
    "        raise ValueError(\"Patch size too small. No patches can be created.\")\n",
    "\n",
    "    return np.array(patches), locations\n",
    "\n",
    "# Extract patches\n",
    "X_patches, X_locations = extract_patches(X, patch_size, stride)\n",
    "y_patches, _ = extract_patches(y, patch_size, stride)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Feature dimensions: {X.shape}, with {X.shape[-1]} features\")\n",
    "print(f\"Target dimensions: {y.shape}\")\n",
    "print(f\"Extracted {len(X_patches)} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "# Data augmentation for model robustness\n",
    "def augment_data(X_data, y_data):\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "\n",
    "    for i in range(len(X_data)):\n",
    "        # Original\n",
    "        X_augmented.append(X_data[i])\n",
    "        y_augmented.append(y_data[i])\n",
    "\n",
    "        # Flip horizontally\n",
    "        X_augmented.append(np.flip(X_data[i], axis=1))\n",
    "        y_augmented.append(np.flip(y_data[i], axis=1))\n",
    "\n",
    "        # Flip vertically\n",
    "        X_augmented.append(np.flip(X_data[i], axis=0))\n",
    "        y_augmented.append(np.flip(y_data[i], axis=0))\n",
    "\n",
    "        # Rotate 90 degrees\n",
    "        X_augmented.append(np.rot90(X_data[i], k=1))\n",
    "        y_augmented.append(np.rot90(y_data[i], k=1))\n",
    "\n",
    "    return np.array(X_augmented), np.array(y_augmented)\n",
    "\n",
    "X_aug, y_aug = augment_data(X_patches, y_patches)\n",
    "print(f\"Dataset size after augmentation: {len(X_aug)} samples\")\n",
    "\n",
    "# Split Data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_aug, y_aug, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc7a36-7c72-4d18-aeb8-5c4dc4a7c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "input_shape = (patch_size, patch_size, X.shape[-1])\n",
    "print(f\"Model input shape: {input_shape}\")\n",
    "\n",
    "# Improved UNet model with batch normalization and dropout\n",
    "inputs = Input(input_shape)\n",
    "\n",
    "# Encoder path with batch normalization\n",
    "c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "c1 = BatchNormalization()(c1)\n",
    "c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "c1 = BatchNormalization()(c1)\n",
    "p1 = MaxPooling2D((2, 2))(c1)\n",
    "p1 = Dropout(0.1)(p1)\n",
    "\n",
    "c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "c2 = BatchNormalization()(c2)\n",
    "c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "c2 = BatchNormalization()(c2)\n",
    "p2 = MaxPooling2D((2, 2))(c2)\n",
    "p2 = Dropout(0.2)(p2)\n",
    "\n",
    "c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "c3 = BatchNormalization()(c3)\n",
    "c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "c3 = BatchNormalization()(c3)\n",
    "p3 = MaxPooling2D((2, 2))(c3)\n",
    "p3 = Dropout(0.3)(p3)\n",
    "\n",
    "c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "c4 = BatchNormalization()(c4)\n",
    "c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "c4 = BatchNormalization()(c4)\n",
    "p4 = MaxPooling2D((2, 2))(c4)\n",
    "p4 = Dropout(0.4)(p4)\n",
    "\n",
    "# Middle\n",
    "c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "c5 = BatchNormalization()(c5)\n",
    "c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "c5 = BatchNormalization()(c5)\n",
    "c5 = Dropout(0.5)(c5)\n",
    "\n",
    "# Decoder path with skip connections\n",
    "u6 = UpSampling2D((2, 2))(c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "c6 = BatchNormalization()(c6)\n",
    "c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "c6 = BatchNormalization()(c6)\n",
    "c6 = Dropout(0.4)(c6)\n",
    "\n",
    "u7 = UpSampling2D((2, 2))(c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "c7 = BatchNormalization()(c7)\n",
    "c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "c7 = BatchNormalization()(c7)\n",
    "c7 = Dropout(0.3)(c7)\n",
    "\n",
    "u8 = UpSampling2D((2, 2))(c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "c8 = BatchNormalization()(c8)\n",
    "c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "c8 = BatchNormalization()(c8)\n",
    "c8 = Dropout(0.2)(c8)\n",
    "\n",
    "u9 = UpSampling2D((2, 2))(c8)\n",
    "u9 = concatenate([u9, c1])\n",
    "c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "c9 = BatchNormalization()(c9)\n",
    "c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "c9 = BatchNormalization()(c9)\n",
    "c9 = Dropout(0.1)(c9)\n",
    "\n",
    "# Output layer\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return 1 - (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Compile with binary cross-entropy + dice loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss=lambda y_true, y_pred: tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred),\n",
    "              metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Set up callbacks for better training\n",
    "checkpoint = ModelCheckpoint('flood_risk_model_best.h5',\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             verbose=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              min_lr=1e-6,\n",
    "                              verbose=1)\n",
    "\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "# Train the model with more epochs and callbacks\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # More epochs with early stopping\n",
    "    batch_size=16,  # Smaller batch size often works better\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bbb46-0bfb-4683-9904-5c6d0970abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flood_risk_model_final.h5')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test loss: {test_results[0]:.4f}\")\n",
    "print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Test recall: {test_results[2]:.4f}\")\n",
    "print(f\"Test precision: {test_results[3]:.4f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_true_flat = y_test.reshape(-1)\n",
    "y_pred_flat = y_pred_binary.reshape(-1)\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true_flat, y_pred_flat, average='binary')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize a few examples\n",
    "plt.figure(figsize=(15, 10))\n",
    "num_samples = min(5, len(X_test))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Original image (show DEM as background)\n",
    "    plt.subplot(3, num_samples, i+1)\n",
    "    plt.imshow(X_test[i][:,:,0], cmap='terrain')\n",
    "    plt.title(f\"Sample {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # True mask\n",
    "    plt.subplot(3, num_samples, i+1+num_samples)\n",
    "    plt.imshow(y_test[i].reshape(patch_size, patch_size), cmap='Blues')\n",
    "    plt.title(f\"True Flood Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Predicted mask\n",
    "    plt.subplot(3, num_samples, i+1+2*num_samples)\n",
    "    plt.imshow(y_pred[i].reshape(patch_size, patch_size), cmap='Blues')\n",
    "    plt.title(f\"Predicted Flood Risk\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_examples.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Flood', 'Flood'],\n",
    "            yticklabels=['No Flood', 'Flood'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555d600-8b7b-4121-ad0b-5559e30ddd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_risk_map(model, X, patch_size, stride):\n",
    "    # Create an empty risk map\n",
    "    height, width = X.shape[0], X.shape[1]\n",
    "    risk_map = np.zeros((height, width))\n",
    "    count_map = np.zeros((height, width))\n",
    "\n",
    "    # Generate predictions for patches across the image\n",
    "    for i in range(0, height - patch_size + 1, stride):\n",
    "        for j in range(0, width - patch_size + 1, stride):\n",
    "            # Extract patch\n",
    "            patch = X[i:i + patch_size, j:j + patch_size, :]\n",
    "\n",
    "            # Make prediction\n",
    "            patch = patch.reshape(1, patch_size, patch_size, X.shape[-1])\n",
    "            pred = model.predict(patch)[0].reshape(patch_size, patch_size)\n",
    "\n",
    "            # Add to risk map and count\n",
    "            risk_map[i:i + patch_size, j:j + patch_size] += pred\n",
    "            count_map[i:i + patch_size, j:j + patch_size] += 1\n",
    "\n",
    "    # Average the overlapping areas\n",
    "    count_map[count_map == 0] = 1  # Avoid division by zero\n",
    "    risk_map = risk_map / count_map\n",
    "\n",
    "    return risk_map\n",
    "\n",
    "# Create the flood risk map\n",
    "flood_risk_map = create_full_risk_map(model, X, patch_size, stride)\n",
    "\n",
    "# Create risk categories\n",
    "risk_categories = np.zeros_like(flood_risk_map)\n",
    "risk_categories[(flood_risk_map >= 0.2) & (flood_risk_map < 0.4)] = 1  # Low risk\n",
    "risk_categories[(flood_risk_map >= 0.4) & (flood_risk_map < 0.6)] = 2  # Medium risk\n",
    "risk_categories[(flood_risk_map >= 0.6) & (flood_risk_map < 0.8)] = 3  # High risk\n",
    "risk_categories[flood_risk_map >= 0.8] = 4  # Very high risk\n",
    "\n",
    "# Save risk map as GeoTIFF\n",
    "if os.path.exists('dem.tif'):\n",
    "    with rasterio.open('dem.tif') as src:\n",
    "        profile = src.profile.copy()\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "        with rasterio.open('flood_risk_map.tif', 'w', **profile) as dst:\n",
    "            dst.write(flood_risk_map.astype(np.float32), 1)\n",
    "\n",
    "        profile.update(dtype=rasterio.uint8, count=1)\n",
    "        with rasterio.open('flood_risk_categories.tif', 'w', **profile) as dst:\n",
    "            dst.write(risk_categories.astype(np.uint8), 1)\n",
    "\n",
    "# Visualize the final risk map\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(flood_risk_map, cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Flood Risk Probability')\n",
    "plt.title('Flood Risk Probability Map')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "cmap = plt.cm.get_cmap('viridis', 5)\n",
    "risk_plot = plt.imshow(risk_categories, cmap=cmap, vmin=0, vmax=4)\n",
    "categories = ['No Risk', 'Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']\n",
    "cbar = plt.colorbar(ticks=range(5))\n",
    "cbar.set_ticklabels(categories)\n",
    "plt.title('Flood Risk Categories')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('flood_risk_map.png')\n",
    "plt.close()\n",
    "\n",
    "# Calculate risk statistics\n",
    "total_area = risk_categories.size\n",
    "risk_stats = {\n",
    "    'No Risk': np.sum(risk_categories == 0) / total_area * 100,\n",
    "    'Low Risk': np.sum(risk_categories == 1) / total_area * 100,\n",
    "    'Medium Risk': np.sum(risk_categories == 2) / total_area * 100,\n",
    "    'High Risk': np.sum(risk_categories == 3) / total_area * 100,\n",
    "    'Very High Risk': np.sum(risk_categories == 4) / total_area * 100\n",
    "}\n",
    "\n",
    "# Print risk statistics\n",
    "print(\"\\nFlood Risk Statistics (% of total area):\")\n",
    "for category, percentage in risk_stats.items():\n",
    "    print(f\"{category}: {percentage:.2f}%\")\n",
    "\n",
    "print(\"\\n✅ Flood risk assessment completed! Risk maps saved as images and GeoTIFF files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
